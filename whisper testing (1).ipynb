{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a5e318-8a2b-4d2e-a915-de3a90b77f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.48.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
      "Requirement already satisfied: silero-vad in /usr/local/lib/python3.10/dist-packages (5.1.2)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
      "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.1.0+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (2024.12.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (2.1.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.15.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.13.0)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: onnxruntime>=1.16.1 in /usr/local/lib/python3.10/dist-packages (from silero-vad) (1.20.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (23.2)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.16.1->silero-vad) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.16.1->silero-vad) (24.12.23)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.16.1->silero-vad) (5.29.3)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (3.11.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2022.12.7)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.16.1->silero-vad) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchaudio) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchaudio) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torchaudio librosa silero-vad pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0fe83ae-425e-414d-9963-0de1d0856863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from silero_vad import load_silero_vad, read_audio, get_speech_timestamps\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import mediainfo\n",
    "\n",
    "import math\n",
    "from io import BytesIO\n",
    "from typing import IO, List\n",
    "\n",
    "\n",
    "def _split_audio(audio_file: IO[bytes], vad_model, sampling_rate) -> List:\n",
    "        \"\"\"\n",
    "        This is a helper function that takes an audio file and splits it into timestamps based on VAD\n",
    "\n",
    "        Parameters:\n",
    "            - audio_file (IO[bytes]) -> Audio file you want to split timestamps of\n",
    "            - vad_model (silero-vad) -> Voice Activity Detection model to analyze the audio.\n",
    "\n",
    "        Returns:\n",
    "            - audio_segments (List) -> List of audio segments (as BytesIO buffers). \n",
    "\n",
    "        TODO: \n",
    "        1. You can try to ceil or floor the timestamps to nearest integer but there could be a problem in recognizing audio if it merges with some random sound or previous syllable. \n",
    "        2. Change from mono to stereo if stereo sound is provided\n",
    "        3. Try changing the export format to mp3 and observe the results\n",
    "        \"\"\"\n",
    "\n",
    "        def split_timestamps(timestamps):\n",
    "            result = []\n",
    "\n",
    "            def find_largest_gap(chunk):\n",
    "                \"\"\"Find the index of the largest gap between consecutive timestamps.\"\"\"\n",
    "                max_gap = 0\n",
    "                split_index = None\n",
    "\n",
    "                for i in range(1, len(chunk)):\n",
    "                    gap = chunk[i]['start'] - chunk[i - 1]['end']\n",
    "                    if gap > max_gap:\n",
    "                        max_gap = gap\n",
    "                        split_index = i\n",
    "\n",
    "                return max_gap, split_index\n",
    "\n",
    "            def process_chunk(chunk):\n",
    "                \"\"\"Process a single chunk and split it if its duration is > 29 seconds.\"\"\"\n",
    "                # Base condition: If chunk has one or fewer timestamps, add it directly\n",
    "                if len(chunk) <= 1:\n",
    "                    result.append(chunk)\n",
    "                    return\n",
    "\n",
    "                # Check duration of the chunk\n",
    "                duration = chunk[-1]['end'] - chunk[0]['start']\n",
    "                if duration <= 29:\n",
    "                    result.append(chunk)  # If duration is <= 29, keep the chunk as is\n",
    "                    return\n",
    "\n",
    "                # Find the largest gap and split at that point\n",
    "                max_gap, split_index = find_largest_gap(chunk)\n",
    "\n",
    "                # If no valid split point is found, add the chunk as is\n",
    "                if max_gap <= 0 or split_index is None:\n",
    "                    result.append(chunk)\n",
    "                    return\n",
    "\n",
    "                # Split the chunk into two at the split_index\n",
    "                left_chunk = chunk[:split_index]\n",
    "                right_chunk = chunk[split_index:]\n",
    "\n",
    "                # Process each sub-chunk recursively\n",
    "                process_chunk(left_chunk)\n",
    "                process_chunk(right_chunk)\n",
    "\n",
    "            # Start processing the chunks\n",
    "            process_chunk(timestamps)\n",
    "\n",
    "            return result\n",
    "\n",
    "        # 1. Read audio and prepare it for processing\n",
    "        audio_bytes = audio_file.getvalue()\n",
    "\n",
    "        # 2. Convert the audio into BytesIO object\n",
    "        audio = BytesIO(audio_bytes)\n",
    "\n",
    "        # 3. Read audio with it's sampling rate\n",
    "        wav = read_audio(audio, sampling_rate=sampling_rate)\n",
    "\n",
    "        # 4. Get speech timestamps from the vad model \n",
    "        speech_timestamps = get_speech_timestamps(\n",
    "            wav, \n",
    "            vad_model, \n",
    "            sampling_rate=sampling_rate, \n",
    "            return_seconds=True\n",
    "        )\n",
    "     \n",
    "        # 5. Extract processed timestamps\n",
    "        processed_timestamps = split_timestamps(speech_timestamps)\n",
    "\n",
    "        # TODO: You can try to ceil or floor the timestamps to nearest integer but there could be a problem in recognizing audio if it merges with some random sound or previous syllable. \n",
    "        cleaned_processed_timestamps = []\n",
    "        for timestamps in processed_timestamps:\n",
    "            cleaned_processed_timestamps.append(\n",
    "                {\n",
    "                    \"start\": math.floor(float(timestamps[0]['start'])), \n",
    "                    \"end\": math.ceil(float(timestamps[-1]['end']))\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # 6. Cleaning processed timestamps\n",
    "        cleaned_processed_timestamps_2 = [cleaned_processed_timestamps[0]]\n",
    "\n",
    "        for i in range(1, len(cleaned_processed_timestamps)):\n",
    "            if (cleaned_processed_timestamps[i]['end']- cleaned_processed_timestamps[i]['start']) + (cleaned_processed_timestamps_2[-1]['end'] - cleaned_processed_timestamps_2[-1]['start']) < 29:\n",
    "                cleaned_processed_timestamps_2[-1]['end'] = cleaned_processed_timestamps[i]['end']\n",
    "            else:\n",
    "                cleaned_processed_timestamps_2.append(cleaned_processed_timestamps[i])\n",
    "\n",
    "        # Convert processed timestamps into audio segments\n",
    "        audio_segments = []\n",
    "        audio = AudioSegment.from_file(audio_file)\n",
    "        for cleaned_processed_timestamp in cleaned_processed_timestamps:\n",
    "            start_ms = cleaned_processed_timestamp['start'] * 1000  # Convert seconds to milliseconds\n",
    "            end_ms = cleaned_processed_timestamp['end'] * 1000    # Convert seconds to milliseconds\n",
    "            segment = audio[start_ms:end_ms]\n",
    "\n",
    "            buffer = BytesIO()\n",
    "            segment.export(buffer, format=\"wav\") # TODO: Try changing it to wav and observe the results\n",
    "            buffer.seek(0)  # Reset buffer pointer\n",
    "            audio_segments.append(buffer)\n",
    "\n",
    "        return audio_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4516cb4-0359-438e-8a23-404dc5d059f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.27.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.5.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.12.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71ddd25c-cfc4-43ce-a96d-0455b853c5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You have passed task=translate, but also have set `forced_decoder_ids` to [[1, None], [2, 50360]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=translate.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  Good morning sir, how are you? Hello Ma'am, which account are you going to in this dividend? Good morning sir, how are you? I am asking you, which account are you going to in this dividend?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  Hello, sir, we are not selling, tell us how are you doing? I am saying that the dividends that go in, will you tell me in which account you will tell me?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  So, it goes to your bank only, if you get any dividend then your particular... I am asking you this particular, you are going now, they can show that yes, they are going to this bank or this bank, what is this? I don't understand, sorry, will you repeat? I am saying this, you can tell which bank they are going to, they will tell you.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  I don't understand, will you repeat? I am saying, can you tell me which band are you going to?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  Yes, I will definitely tell you. Rajesh, you have registered banks here. You have registered 4 banks. One is Punjab National Bank, State Bank of India.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  And one is your Indian bank, Indian bank, State bank of India, Yash bank, these four banks you have added. In this, it is going to your primary. Primary bank I told you which was the primary. Primary bank you have kept Punjab National Bank, so your dividend is going in this. And see, if I go to close two banks in Mali's investment, then it will go away.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  Yes, you will get removed from the bank application, you have to keep one minimum, you can get removed from that. I am not getting removed from the app, I am not getting removed from the app. Are you not getting the default option from the app? I am getting it but not getting removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  Actually, you are not getting the option of default, right? I am getting it but not getting it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  Ok, I will request you if the bank is not being removed from there, then we will check out our team and ask them why it is not being removed from there. And they will help you for that. So once you give this mail. This is not for both of them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  So, both of them... You want to move? Yes. It's okay, it will move. Yes, bank. It's okay, it will move. You can mail it once.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  Yes Venk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  Then you will get rid of the thyroid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  Ok, I have to mail it now. Yes, please mail it now. You can't do it on the phone. I am really sorry. On-call is not possible. Really sorry. You don't have to do it. Mail id is our customer care at the rate choice will be your outcome. Ok, I will send it. Ok, bye.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  Customer care at the rate of 24.5 Sir, your voice echo is happening, but if you...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "# from datasets import load_dataset\n",
    "import librosa\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "\n",
    "# Set up device and data types\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Load model and processor\n",
    "model_id = \"openai/whisper-large-v3\"  # Use your custom Whisper model\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Create ASR pipeline\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "\n",
    "# Load your audio file\n",
    "audio_file = \"./recordingsv4/7f5662f8-e39c-4429-b7a4-39628fd750a1.mp3\"\n",
    "vad_model = load_silero_vad()  # Implement your VAD logic here\n",
    "\n",
    "audio = open(audio_file, \"rb\")\n",
    "audio = BytesIO(audio.read())\n",
    "\n",
    "# Load the audio and apply Voice Activity Detection (VAD) \n",
    "waveform, sampling_rate = librosa.load(audio_file, sr=None)\n",
    "audio_segments = _split_audio(audio, vad_model, sampling_rate)  # Split audio into segments\n",
    "\n",
    "# Transcribe each audio segment\n",
    "for segment in audio_segments:\n",
    "    # Load audio segment\n",
    "    audio_segment = AudioSegment.from_file(segment)\n",
    "\n",
    "    # Convert Pydub AudioSegment to NumPy array\n",
    "    samples = np.array(audio_segment.get_array_of_samples(), dtype=np.float32)\n",
    "    samples /= np.iinfo(audio_segment.array_type).max  # Normalize to [-1, 1]\n",
    "\n",
    "    # Resample to 16 kHz\n",
    "    waveform = librosa.resample(samples, orig_sr=audio_segment.frame_rate, target_sr=16000)\n",
    "    generate_kwargs = {\n",
    "    \"max_new_tokens\": 445,\n",
    "    \"language\": \"hindi\", \n",
    "    \"task\": \"translate\",\n",
    "    \"num_beams\": 1,\n",
    "    \"condition_on_prev_tokens\": True,\n",
    "    \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n",
    "    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n",
    "    \"logprob_threshold\": -1.0,\n",
    "    \"no_speech_threshold\": 0.6,\n",
    "    \"return_timestamps\": True,\n",
    "    }\n",
    "\n",
    "    result = pipe(waveform, generate_kwargs=generate_kwargs)\n",
    "    # Run transcription pipeline\n",
    "    # result = pipe({\"array\": waveform, \"sampling_rate\": 16000})\n",
    "    print(\"Transcription:\", result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82f8ce83-2d43-40d4-a050-b3e30deb5a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "# from pydub import AudioSegment\n",
    "# import librosa\n",
    "# import numpy as np\n",
    "# from io import BytesIO\n",
    "# import torch\n",
    "\n",
    "# # Check if GPU is available\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # Load custom model and processor\n",
    "# model_name = \"quinnb/whisper-Large-v3-hindi\"  # Replace with your desired model\n",
    "# processor = WhisperProcessor.from_pretrained(model_name)\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "# model.config.forced_decoder_ids = None\n",
    "\n",
    "# # Load audio file\n",
    "# audio_file = \"./recordingsv4/f83d924c-8c14-48d2-af37-b4e71413490a.mp3\"\n",
    "# with open(audio_file, \"rb\") as f:\n",
    "#     audio = f.read()\n",
    "# audio = BytesIO(audio)\n",
    "\n",
    "# # Load Silero VAD (adjust based on your custom `_split_audio` logic)\n",
    "# vad_model = load_silero_vad()  # Ensure you have a valid implementation for this function\n",
    "\n",
    "# # Process the audio\n",
    "# waveform, sampling_rate = librosa.load(audio_file, sr=None)  # Load audio and retain original sampling rate\n",
    "# audio_segments = _split_audio(audio, vad_model, sampling_rate)  # Split into segments\n",
    "\n",
    "# # Process each segment and transcribe\n",
    "# for audio_segment in audio_segments:\n",
    "#     # Load the audio segment into a Pydub AudioSegment object\n",
    "#     audio_segment = AudioSegment.from_file(audio_segment)\n",
    "\n",
    "#     # Convert AudioSegment to NumPy array\n",
    "#     samples = np.array(audio_segment.get_array_of_samples(), dtype=np.float32)\n",
    "#     samples /= np.iinfo(audio_segment.array_type).max  # Normalize to range [-1, 1]\n",
    "\n",
    "#     # Resample audio to 16 kHz\n",
    "#     sampling_rate = audio_segment.frame_rate\n",
    "#     waveform = librosa.resample(samples, orig_sr=sampling_rate, target_sr=16000)\n",
    "\n",
    "#     # Generate input features for Whisper\n",
    "#     input_features = processor(waveform, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "#     input_features = input_features.to(device)  # Move input features to GPU\n",
    "\n",
    "#     # Generate transcription\n",
    "#     predicted_ids = model.generate(input_features)\n",
    "#     transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "#     print(\"Transcription:\", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8409995-2561-4a21-a079-7143b677c266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c260d-8f70-4754-95c7-5010f5f9d45b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
